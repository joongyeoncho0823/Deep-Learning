{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mSi5iWwUgKMK"},"outputs":[],"source":["# Simon Yoon, Steven Cho\n","# DL Midterm Project\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","import keras\n","from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, Activation, MaxPooling2D\n","from keras.datasets.cifar10 import load_data\n","from tensorflow.keras.losses import sparse_categorical_crossentropy\n","from tensorflow.keras.optimizers import RMSprop\n","import matplotlib.pyplot as plt\n","\n","# As specified in paper\n","MICROBATCH_SIZE = 4\n","NUM_MICROBATCHES = 400\n","BATCH_SIZE = MICROBATCH_SIZE * NUM_MICROBATCHES\n","NUM_EPOCHS = 2\n","\n","NUM_CLASSES = 100\n","HEIGHT = 32\n","WIDTH = 32\n","NUM_CHANNELS = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2iB1AGEgNP6"},"outputs":[],"source":["from keras import backend as K\n","from keras.layers import Layer\n","\n","class CustomBatchNorm(Layer):\n","\n","    def __init__(\n","        self,\n","        renorm_clipping,\n","        r_max_value=3,\n","        d_max_value=5,\n","        t_delta=1e-3,\n","        renorm=False,\n","        renorm_momentum=0.99\n","        ):\n","        super(CustomBatchNorm, self).__init__()\n","        self.epsilon = 1e-4\n","        renorm_clipping = renorm_clipping or {}\n","        keys = [\"rmax\", \"rmin\", \"dmax\"]\n","        if set(renorm_clipping) - set(keys):\n","            raise ValueError(\n","                \"Received invalid keys for `renorm_clipping` argument: \"\n","                f\"{renorm_clipping}. Supported values: {keys}.\"\n","            )\n","\n","        self.renorm = renorm\n","        self.renorm_clipping = renorm_clipping\n","        self.renorm_momentum = renorm_momentum\n","        #self.r = 1 # default\n","        #self.d = 0\n","        self.r_max_value = r_max_value\n","        self.d_max_value = d_max_value\n","        self.t_delta = t_delta\n","\n","    def build(self, input_shape):\n","        input_shape = tf.TensorShape(input_shape)\n","        rank = input_shape.rank\n","\n","        self.beta = self.add_weight(\n","            shape=(input_shape[-1]),\n","            initializer=\"zeros\",\n","            trainable=True,dtype = tf.float32\n","        )\n","\n","        self.gamma = self.add_weight(\n","            shape=(input_shape[-1]),\n","            initializer=\"ones\",\n","            trainable=True,dtype = tf.float32\n","        )\n","\n","        self.moving_mean = self.add_weight(\n","            shape=(input_shape[-1]),\n","            initializer=tf.initializers.zeros,\n","            trainable=False, dtype = tf.float32)\n","\n","        self.moving_variance = self.add_weight(\n","            shape=(input_shape[-1]),\n","            initializer=tf.initializers.ones,\n","            trainable=False, dtype = tf.float32)\n","        \n","    def get_moving_average(self, statistic, new_value):\n","        alpha = 1-self.renorm_momentum\n","        new_value = statistic*self.renorm_momentum + (tf.reduce_mean(new_value, [0,1,2,3]) - statistic)*(alpha)\n","        return new_value\n","    '''\n","    def normalise(self, x, x_mean, x_var, r, d):\n","        return (x - x_mean) / tf.sqrt(x_var + self.epsilon) * self.r + self.d\n","    '''\n","\n","    #@tf.function\n","    def call(self, inputs, training):\n","        input_shape = tf.shape(inputs)\n","        # Original Image Dimension: [N, H, W, C]\n","        # Reshaped image Dimension: [N, M, H, W, C] // N = NUM_MICROBATCHES, M = MICROBATCH_SIZE\n","        inputs_shaped = tf.reshape(inputs, [\n","            input_shape[0]/MICROBATCH_SIZE,\n","            MICROBATCH_SIZE,\n","            input_shape[1],\n","            input_shape[2],\n","            input_shape[3]\n","      ])\n","        r,d = 1,0\n","        if training:\n","            axes  = [1, 2, 3] # Specific for project\n","            mean, var = tf.nn.moments(inputs_shaped, axes=axes, keepdims=True)\n","            \n","            #self.moving_mean.assign(self.get_moving_average(self.moving_mean, mean))\n","            #self.moving_variance.assign(self.get_moving_average(self.moving_variance, var))\n","            \n","            if self.renorm:\n","                \n","                std_batch = tf.sqrt(var + self.epsilon)\n","                #self.r.assign(tf.stop_gradient(tf.clip_by_value(tf.cast(std_batch / tf.sqrt(self.moving_variance + self.epsilon), tf.float32), self.renorm_clipping['rmin'], self.renorm_clipping['rmax'])))\n","                #self.d.assign(tf.stop_gradient(tf.clip_by_value(tf.cast(mean-self.moving_mean,tf.float32), -self.renorm_clipping['dmax'], self.renorm_clipping['dmax'])))\n","                #self.r.assign(1)\n","                #self.d.assign(0)\n","                r = std_batch / (K.sqrt(self.moving_variance + self.epsilon))\n","                r = K.stop_gradient(K.clip(r, 1 / self.renorm_clipping['rmax'], self.renorm_clipping['rmax']))\n","                d = (mean - self.moving_mean) / K.sqrt(self.moving_variance\n","                                                      + self.epsilon)\n","                d = K.stop_gradient(K.clip(d, -self.renorm_clipping['dmax'], self.renorm_clipping['dmax']))\n","\n","            # mean_update = K.moving_average_update(self.moving_mean,\n","            #                                   tf.reduce_mean(mean, [0,1,2,3]),\n","            #                                   self.renorm_momentum)\n","            # variance_update = K.moving_average_update(self.moving_variance,\n","            #                                       tf.reduce_mean(std_batch ** 2, [0,1,2,3]),\n","            #                                       self.renorm_momentum)\n","            # self.add_update([mean_update, variance_update], inputs)\n","            self.moving_mean.assign(self.get_moving_average(self.moving_mean, mean))\n","            self.moving_variance.assign(self.get_moving_average(self.moving_variance, var))\n","            # r_val = self.r_max_value / (1 + (self.r_max_value - 1) * K.exp(-self.t))\n","            # d_val = (self.d_max_value\n","            #      / (1 + ((self.d_max_value / 1e-3) - 1) * K.exp(-(2 * self.t))))\n","\n","            # self.add_update([K.update(self.r_max, r_val),\n","            #              K.update(self.d_max, d_val),\n","            #              K.update_add(self.t, self.t_delta_tensor)], inputs)    \n","        else:\n","                mean, var = self.moving_mean, self.moving_variance\n","        \n","        x = (inputs_shaped - mean) / tf.sqrt(var + self.epsilon) * r + d\n","        return tf.reshape(self.gamma * x + self.beta, input_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIE9fBEYgPOO"},"outputs":[],"source":["# Load Data\n","(x_train, y_train), (x_test, y_test) = load_data()\n","\n","# Parse numbers as floats\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","# Normalization\n","x_train = x_train / 255.0\n","x_test = x_test / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZnL28ophymV"},"outputs":[],"source":["# For RAM\n","x_train = x_train[0:6400]\n","y_train = y_train[0:6400]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ui0gUEddgRUZ"},"outputs":[],"source":["curr_epoch = 0\n","\n","class RenormCallback(tf.keras.callbacks.Callback):\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        global curr_epoch\n","        # if not self.check_condition(epoch):\n","        #     return\n","        curr_epoch = curr_epoch + 1\n","\n","def get_rmax(num_epoch):\n","    thresh_epoch = 20\n","    if num_epoch < thresh_epoch:\n","        return 1\n","    else:\n","        return 1 + 2*(num_epoch)/(NUM_EPOCHS)\n","\n","\n","def get_dmax(num_epoch):\n","    thresh_epoch = 20\n","    if num_epoch < thresh_epoch:\n","        return 0\n","    else:\n","        return 6 * (num_epoch / NUM_EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1093,"status":"ok","timestamp":1666996442070,"user":{"displayName":"Joongyeon Cho","userId":"18088951130262025831"},"user_tz":240},"id":"Z527daC_gS20","outputId":"4a5faefc-ba9d-48fc-ce23-a0d54f99a6ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n","                                                                 \n"," custom_batch_norm_4 (Custom  (None, 32, 32, 32)       131       \n"," BatchNorm)                                                      \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 32, 32, 32)        9248      \n","                                                                 \n"," custom_batch_norm_5 (Custom  (None, 32, 32, 32)       131       \n"," BatchNorm)                                                      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32, 32, 32)        0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 32, 32, 64)        18496     \n","                                                                 \n"," custom_batch_norm_6 (Custom  (None, 32, 32, 64)       259       \n"," BatchNorm)                                                      \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 32, 32, 64)        36928     \n","                                                                 \n"," custom_batch_norm_7 (Custom  (None, 32, 32, 64)       259       \n"," BatchNorm)                                                      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 32, 32, 64)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 65536)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 100)               6553700   \n","                                                                 \n","=================================================================\n","Total params: 6,620,048\n","Trainable params: 6,619,664\n","Non-trainable params: 384\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import Input\n","\n","input_shape = (HEIGHT,WIDTH,NUM_CHANNELS)\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001), input_shape=input_shape))\n","model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","model.add(Conv2D(32, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(64, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","model.add(Conv2D(64, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","model.add(Dropout(0.3))\n","\n","# model.add(Conv2D(128, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","# model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","# model.add(Conv2D(128, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","# model.add(CustomBatchNorm(renorm= True, renorm_clipping = {'rmax':get_rmax(curr_epoch),'rmin':1/get_rmax(curr_epoch),'dmax':get_dmax(curr_epoch)}))\n","# model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(NUM_CLASSES, activation=tf.nn.softmax))\n","\n","\n","# Compile the model\n","model.compile(loss=sparse_categorical_crossentropy,\n","              optimizer=RMSprop(),\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265049,"status":"ok","timestamp":1666996708983,"user":{"displayName":"Joongyeon Cho","userId":"18088951130262025831"},"user_tz":240},"id":"ZNaV7V6vgVgE","outputId":"f68ac580-4ff6-4b00-b036-4a97b0cbc4da"},"outputs":[],"source":["# Fit data to model\n","history = model.fit(x_train, y_train,\n","            batch_size=BATCH_SIZE,\n","            epochs=NUM_EPOCHS,\n","            validation_split = 0.2,\n","            verbose = 1,\n","            callbacks = [RenormCallback()]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1666996828678,"user":{"displayName":"Joongyeon Cho","userId":"18088951130262025831"},"user_tz":240},"id":"xN1OFZzggX1e","outputId":"a20e5776-288e-4354-f27c-10b5b23f1519"},"outputs":[],"source":["from tensorflow.keras.layers import BatchNormalization\n","\n","input_shape = (HEIGHT,WIDTH,NUM_CHANNELS)\n","\n","model = Sequential()\n","model.add(Input(\n","    shape=input_shape,\n","    batch_size=BATCH_SIZE))\n","model.add(Conv2D(32, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Conv2D(32, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(64, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.3))\n","\n","model.add(Conv2D(128, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128, (3, 3), activation='elu', padding='same', kernel_regularizer=tf.keras.regularizers.l2(.0001)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(NUM_CLASSES, activation=tf.nn.softmax))\n","\n","\n","\n","# Compile the model\n","model.compile(loss=sparse_categorical_crossentropy,\n","              optimizer=RMSprop(),\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"executionInfo":{"elapsed":183490,"status":"error","timestamp":1666997014991,"user":{"displayName":"Joongyeon Cho","userId":"18088951130262025831"},"user_tz":240},"id":"xRGT8W3qgZXC","outputId":"f5d5efd1-9367-470c-8979-5482fd1b9f62"},"outputs":[],"source":["# Fit data to model\n","history2 = model.fit(x_train, y_train,\n","            batch_size=BATCH_SIZE,\n","            epochs=NUM_EPOCHS,\n","            validation_split = 0.2,\n","            verbose = 1\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":30143,"status":"error","timestamp":1666992541966,"user":{"displayName":"Joongyeon Cho","userId":"18088951130262025831"},"user_tz":240},"id":"0XlBlbhFfv96","outputId":"7fdb90be-ddb2-492b-d06a-cfee125837dc"},"outputs":[],"source":["# Plot the history\n","y1=history.history['val_accuracy']\n","y2=history2.history['val_accuracy']\n","x1 = np.arange(len(y1))\n","k=len(y1)/len(y2)\n","x2 = np.arange(k,len(y1)+1,k)\n","fig, ax = plt.subplots()\n","line1, = ax.plot(x1, y1, 'r',label='Batch Renormalization')\n","line2, = ax.plot(x2, y2, 'b--', label='Batch Normalization')\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend(loc=4)\n","plt.show()\n","plt.savefig(\"batchrenorm.png\")\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
